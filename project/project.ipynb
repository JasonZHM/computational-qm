{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Model for Lattice Field Theory\n",
    "\n",
    "Author: Haimeng Zhao\n",
    "\n",
    "Email: haimengzhao@icloud.com\n",
    "\n",
    "This notebook contains the code and a detailed note for utilizing flow-based models to sample from complicated probability distributions, especially those encountered in many-body systems. \n",
    "\n",
    "We first introduce the flow model in a general setting, and then use the flow model to study the lattice $\\phi^4$ theory as an example.\n",
    "\n",
    "The method implemented here is based on several papers ([arXiv:1904.12072](https://inspirehep.net/literature/1731778), [arXiv:2002.02428](https://inspirehep.net/literature/1779199), and [arXiv:2003.06413](https://inspirehep.net/literature/1785309)) and a tutorial [arXiv:2101.08176](https://arxiv.org/abs/2101.08176). \n",
    "\n",
    "We first import some useful packages and check whether GPUs are available (if not, CPUs will be used instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Use CPU or GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = 'cuda'\n",
    "    float_dtype = np.float32 # single\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    torch_device = 'cpu'\n",
    "    float_dtype = np.float64 # double\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(f\"TORCH DEVICE: {torch_device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we borrow some useful functions from 2101.08176.\n",
    "\n",
    "- `grab` is used to move tensors to cpu. \n",
    "\n",
    "- `init_live_plot, moving_average, update_plots` is used to make live-updating plots for monitoring training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ref: 2101.08176\n",
    "'''\n",
    "\n",
    "def grab(var):\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def init_live_plot(dpi=125, figsize=(8,4), N_era=25, N_epoch=100):\n",
    "    fig, ax_ess = plt.subplots(1, 1, dpi=dpi, figsize=figsize)\n",
    "    plt.xlim(0, N_era * N_epoch)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    ess_line = plt.plot([0], [0], alpha=0.5) # dummy\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('ESS')\n",
    "    \n",
    "    ax_loss = ax_ess.twinx()\n",
    "    loss_line = plt.plot([0], [0], alpha=0.5, c='orange') # dummy\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    display_id = display(fig, display_id=True)\n",
    "\n",
    "    return dict(\n",
    "        fig=fig, ax_ess=ax_ess, ax_loss=ax_loss,\n",
    "        ess_line=ess_line, loss_line=loss_line,\n",
    "        display_id=display_id\n",
    "    )\n",
    "\n",
    "def moving_average(x, window=10):\n",
    "    if len(x) < window:\n",
    "        return np.mean(x, keepdims=True)\n",
    "    else:\n",
    "        return np.convolve(x, np.ones(window), 'valid') / window\n",
    "\n",
    "def update_plots(history, fig, ax_ess, ax_loss, ess_line, loss_line, display_id):\n",
    "    Y = np.array(history['ess'])\n",
    "    Y = moving_average(Y, window=15)\n",
    "    ess_line[0].set_ydata(Y)\n",
    "    ess_line[0].set_xdata(np.arange(len(Y)))\n",
    "    Y = history['loss']\n",
    "    Y = moving_average(Y, window=15)\n",
    "    loss_line[0].set_ydata(np.array(Y))\n",
    "    loss_line[0].set_xdata(np.arange(len(Y)))\n",
    "    ax_loss.relim()\n",
    "    ax_loss.autoscale_view()\n",
    "    fig.canvas.draw()\n",
    "    display_id.update(fig) # need to force colab to update plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The essential idea behind this method is to use a neural network to learn the transformation from a simple distribution (the prior), which can be easily sampled, to the complicated distribution we target at (see the figure below from 1904.12072).\n",
    "\n",
    "![Fig. 1 of 1904.12072](./assets/normalizing_flow.png)\n",
    "\n",
    "For example, we can choose the prior to be a simple normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalPrior:\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = torch.flatten(mean)\n",
    "        self.var = torch.flatten(var)\n",
    "        self.dist = torch.distributions.normal.Normal(self.mean, self.var)\n",
    "        self.shape = mean.shape\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        logp = self.dist.log_prob(x.reshape(x.shape[0], -1))\n",
    "        return torch.sum(logp, dim=1)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        x = self.dist.sample((batch_size,))\n",
    "        return x.reshape(batch_size, *self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here for the purpose of lattice field theory, we assume the target density $p(x)$ to be $e^{-S}/Z$, where $S$ is the action of the field theory and $Z$ the normalization constant.\n",
    "\n",
    "When the neural network successfully learned the desired transformation, we can then sample from the prior and apply the neural network to map our samples to the desired distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flow_to_prior(prior, layers, batch_size):\n",
    "    # sample from prior\n",
    "    x = prior.sample(batch_size)\n",
    "    logq = prior.log_prob(x)\n",
    "\n",
    "    # flow through the model\n",
    "    for l in layers:\n",
    "        x, logJ = l.forward(x)\n",
    "        logq = logq - logJ\n",
    "    \n",
    "    return x, logq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neural network represents a gradual \"flow\" of distribution from the prior to the target, it is called a flow model. To further stress that each slice of the flow is a distribution itself which satisfies the normalizing condition $\\int p(x) dx = 1$, the model is often called a normalizing flow.\n",
    "\n",
    "However, the trained neural network is only an approximation, thus the samples drawn directly through the flow is biased. To produce unbiased samples and thus measure observables, we still need resample precedures such as Markov chain Monte Carlo (MCMC). (If you only need an approximation, direct sampling through the flow will be enough).\n",
    "\n",
    "But this time MCMC will require significantly less burn-in time and is more robust to model parameters thanks to the flow. Furthermore, sampling with the flow is also free from the fermion sign problem (though it requires the flow to have a property called \"equivariant\", which happens to be met by the well-known transformer model from the field of natural language processing). In addition, since flow can be run in parrallel, MCMC will be significantly faster. Thus the resulting MCMC-flow hybrid sampler is much better than a simple MCMC.\n",
    "\n",
    "To quantify the quality of flow model samples, we can use the effective sample size (ESS) (which serves a similar role as autocorrelation time in MCMC)\n",
    "$$\n",
    "ESS = \\frac{ \\left(\\frac{1}{N} \\sum_i p(x_i)/q(x_i) \\right)^2 }{ \\frac{1}{N} \\sum_i \\left( p(x_i)/q(x_i) \\right)^2 } \\in [0, 1].\n",
    "$$\n",
    "Here a larger ESS indicates better sampling, and $ESS=1$ represents a perfect sampling from the desired distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2 * torch.logsumexp(logw, dim=0) - torch.logsumexp(2 * logw, dim=0)\n",
    "    return torch.exp(log_ess) / len(logw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we borrow the MCMC code from 2101.08176."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ref: 2101.08176\n",
    "'''\n",
    "\n",
    "def serial_sample_generator(model, action, batch_size, N_samples):\n",
    "    '''\n",
    "    Generate samples from prior and flow through the model, \n",
    "    yield logq and logp alongside.\n",
    "\n",
    "    Here for lattice field theory purpose, we assume p = e^{-S}/Z, \n",
    "    where S is the action of the field theory.\n",
    "    '''\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    layers.eval()\n",
    "    x, logq, logp = None, None, None\n",
    "    for i in range(N_samples):\n",
    "        batch_i = i % batch_size\n",
    "        if batch_i == 0:\n",
    "            # we're out of samples to propose, generate a new batch\n",
    "            x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "            logp = -action(x)\n",
    "        yield x[batch_i], logq[batch_i], logp[batch_i]\n",
    "    \n",
    "def make_mcmc_ensemble(model, action, batch_size, N_samples):\n",
    "    history = {\n",
    "        'x' : [],\n",
    "        'logq' : [],\n",
    "        'logp' : [],\n",
    "        'accepted' : []\n",
    "    }\n",
    "\n",
    "    # build Markov chain\n",
    "    sample_gen = serial_sample_generator(model, action, batch_size, N_samples)\n",
    "    for new_x, new_logq, new_logp in sample_gen:\n",
    "        if len(history['logp']) == 0:\n",
    "            # always accept first proposal, Markov chain must start somewhere\n",
    "            accepted = True\n",
    "        else: \n",
    "            # Metropolis acceptance condition\n",
    "            last_logp = history['logp'][-1]\n",
    "            last_logq = history['logq'][-1]\n",
    "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq))\n",
    "            p_accept = min(1, p_accept)\n",
    "            draw = torch.rand(1) # ~ [0,1]\n",
    "            if draw < p_accept:\n",
    "                accepted = True\n",
    "            else:\n",
    "                accepted = False\n",
    "                new_x = history['x'][-1]\n",
    "                new_logp = last_logp\n",
    "                new_logq = last_logq\n",
    "        # Update Markov chain\n",
    "        history['logp'].append(new_logp)\n",
    "        history['logq'].append(new_logq)\n",
    "        history['x'].append(new_x)\n",
    "        history['accepted'].append(accepted)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Flow\n",
    "\n",
    "Now we dive into the detail of normalizing flow.\n",
    "\n",
    "Translating the above intuition into rigorous math, we aim to find a transformation $f(z)$ which maps a random variable $z$ with a simple prior density $r(z)$ to the ouput variable $x = f(z)$ with density $q(x)$. By the change-of-variable formula we have\n",
    "\n",
    "$$\n",
    "q(x) = r(z)|J|^{-1} = r(z)\\left |\\det \\frac{\\partial f_i(z)}{\\partial z_j}\\right |^{-1},\n",
    "$$\n",
    "\n",
    "where $J = \\det \\frac{\\partial f_i(z)}{\\partial z_j}$ is the Jacobian. \n",
    "\n",
    "That's all it is! A change of variable. Now we only need to train a neural network to find the optimal $f$ which minimizes the distance between the output density $q(x)$ and the target density $p(x)$:\n",
    "$$\n",
    "f = \\argmin_f d(q, p).\n",
    "$$\n",
    "\n",
    "A common choice use to measure the distance between two distributions is the Kullback-Leibler (KL) divergence\n",
    "$$\n",
    "D_{KL}(q||p) = \\int dx \\ q(x)[\\log q(x) - \\log p(x)],\n",
    "$$\n",
    "which can be estimated by\n",
    "$$\n",
    "\\hat{D}_{KL}(q||p) = \\frac{1}{N}\\sum_i^N[\\log q(x_i) - \\log p(x_i)], \\quad x_i \\sim q.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(logp, logq):\n",
    "    return torch.mean(logq - logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the advantage of flow models compared with traditional method. We only need to sample from the \"model distribution\" $q(x)$, which can be generated easily from the prior, while traditional methods such as HMC require sampling from $p(x)$.\n",
    "\n",
    "To make it short, our training procedure consists of\n",
    "1. Drawing samples from the prior and flow through the model,\n",
    "2. Estimate the KL divergence,\n",
    "3. Use optimization methods such as SGD or Adam to minimize the KL divergence.\n",
    "\n",
    "During training, we monitor the KL divergence (loss function) and ESS to keep track of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, action, optimizer, batch_size, metrics):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, logq = apply_flow_to_prior(prior, layers, batch_size)\n",
    "    logp = -action(x)\n",
    "    loss = kl_divergence(logp, logq)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    metrics['loss'].append(grab(loss))\n",
    "    metrics['logp'].append(grab(logp))\n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab(ess(logp, logq)))\n",
    "\n",
    "def print_metrics(era, epoch, history, avg_last_N_epochs):\n",
    "    print(f'== Era {era} | Epoch {epoch} metrics ==')\n",
    "    for key, val in history.items():\n",
    "        avgd = np.mean(val[-avg_last_N_epochs:])\n",
    "        print(f'\\t{key} {avgd:g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are left with now is how to design a flow $f$ that is expressive enough while keeping its Jacobian tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the flow $f$\n",
    "\n",
    "To design a flow that is both expressive enough and tractable, people have came up with different solutions. Two notable examples are coupling layers and Monge-Ampere flow (see arXiv:1809.10188 where the authors demonstrated this approach in MNIST generation and Ising Model sampling). Here we will focus on coupling layers following arXiv:1904.12072. Nevertheless, Monge-Ampere flow might achieve better performance since it's in some sense the continuous version of coupling layers.\n",
    "\n",
    "Coupling layers aims at constructing transformation whose Jacobian matrix is triangular, of which the determinant can be easily read off.\n",
    "\n",
    "In a coupling layer, a subset of the parameters is transformed by a manifestly invertable function such as affine transformation $x\\to e^s x+t$. For example\n",
    "\\begin{align*}\n",
    "x_1' &= e^{-s(x_2)}x_1,\\\\\n",
    "x_2' &= x_2.\n",
    "\\end{align*}\n",
    "Its Jacobian is simply\n",
    "$$\n",
    "J = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial x_1'}{\\partial x_1} & \\frac{\\partial x_1'}{\\partial x_2}\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "and its determinant easily follows\n",
    "$$\n",
    "\\det J = \\prod_i e^{[s(x_2)]_i}.\n",
    "$$\n",
    "\n",
    "By design powerful enough function $s(x)$ (e.g. use a CNN), and composing lots of coupling layers, we can make the flow more expressive. Composing lots of layers gives\n",
    "$$\n",
    "q(x) = r(z)\\left |\\det \\frac{\\partial f_i(z)}{\\partial z_j}\\right |^{-1} = r(z)\\prod_l J^{-1}_l,\n",
    "$$\n",
    "where $J_l$ is the Jacobian of the $l$ th layer.\n",
    "\n",
    "To partition variables into $x_1$ and $x_2$, we adopt the checkerboard mask. Those assigned $1$ will be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example mask:\n",
      " tensor([[1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def get_mask(shape, parity):\n",
    "    mask = torch.ones(shape, dtype=torch.uint8)\n",
    "    mask -= parity\n",
    "    mask[::2, ::2] = parity\n",
    "    mask[1::2, 1::2]= parity\n",
    "    return mask.to(torch_device)\n",
    "\n",
    "print('example mask:\\n', get_mask((8, 8), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our coupling layer.\n",
    "\n",
    "Note that the reverse process is just as simple as the forward one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(torch.nn.Module):\n",
    "    def __init__(self, net, mask_shape, mask_parity):\n",
    "        super().__init__()\n",
    "        self.mask = get_mask(mask_shape, mask_parity)\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = self.mask * x      \n",
    "        x_1 = (1 - self.mask) * x\n",
    "        func = self.net(x_2.unsqueeze(1))\n",
    "        s, t = func[:,0], func[:,1]\n",
    "        fx = (1 - self.mask) * t + x_1 * torch.exp(s) + x_2\n",
    "        axes = range(1, len(s.size()))\n",
    "        logJ = torch.sum((1 - self.mask) * s, dim=tuple(axes))\n",
    "        return fx, logJ\n",
    "\n",
    "    def reverse(self, fx):\n",
    "        fx_2 = self.mask * fx\n",
    "        fx_1 = (1 - self.mask) * fx  \n",
    "        func = self.net(fx_2.unsqueeze(1))\n",
    "        s, t = func[:,0], func[:,1]\n",
    "        x = (fx_1 - (1 - self.mask) * t) * torch.exp(-s) + fx_2\n",
    "        axes = range(1,len(s.size()))\n",
    "        logJ = torch.sum((1 - self.mask)*(-s), dim=tuple(axes))\n",
    "        return x, logJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CNN as the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN(hidden_sizes, kernel_size, in_channels, out_channels):\n",
    "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
    "    padding_size = (kernel_size // 2)\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        layers.append(torch.nn.Conv2d(sizes[i], sizes[i+1], kernel_size, padding=padding_size, stride=1, padding_mode='circular'))\n",
    "        if i != len(sizes) - 2:\n",
    "            layers.append(torch.nn.PReLU())\n",
    "        else:\n",
    "            layers.append(torch.nn.Tanh())\n",
    "    return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tools at hand, we can finally assemble a flow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow(n_layers, lattice_shape, hidden_sizes, kernel_size):\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        parity = i % 2\n",
    "        net = get_CNN(hidden_sizes, kernel_size, in_channels=1, out_channels=2, kernel_size=kernel_size)\n",
    "        coupling_layer = CouplingLayer(net, lattice_shape, parity)\n",
    "        layers.append(coupling_layer)\n",
    "    return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 2-dim lattice $\\phi^4$ theory\n",
    "\n",
    "Now we take 2-dim lattice $\\phi^4$ theory as an example to illustrate the flow model. This part follows arXiv:1904.12072.\n",
    "\n",
    "$phi^4$ field theory assign a real value $\\phi(x)$ to each coordinate $x$ and follows the Boltzmann distribution\n",
    "$$\n",
    "p = \\frac{1}{Z}e^{-S(\\phi)},\n",
    "$$\n",
    "where $Z$ is the normalizing constant and $S$ is the action.\n",
    "\n",
    "In continuous 2-dim theory, the $\\phi^4$ action reads\n",
    "$$\n",
    "S[\\phi] = \\int d^2x (\\partial \\phi)^2 + m^2\\phi^2 + \\lambda \\phi^4.\n",
    "$$\n",
    "Discretizing it on a lattice gives\n",
    "$$\n",
    "S(\\phi) = \\sum_n \\phi(n) ()\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
